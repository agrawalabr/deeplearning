{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agrawalabr/deeplearning/blob/main/Vision%20Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformers(VIT) | Transformers in Computer Vision\n",
        "\n",
        "Transformer architectures, originally designed for Natural Language Processing (NLP), have revolutionized the field of deep learning, forming the backbone of state-of-the-art models across various NLP applications. More recently, these architectures have been successfully adapted to computer vision tasks, leading to a new paradigm in image processing.\n",
        "\n",
        "The [Vision Transformer (ViT)](https://arxiv.org/pdf/2010.11929), gained large attention in 2021, demonstrates how standard transformer architectures can achieve competitive performance on image classification tasks. The core idea is to split an image into smaller patches, treat each patch as a token, and process them using a sequence of self-attention-based transformer blocks. This approach eliminates the need for traditional convolutional neural networks (CNNs), offering a more flexible and scalable framework for vision tasks.\n",
        "\n",
        "However, working with ViTs presents a few challenges:\n",
        "- Computational Complexity: ViTs require significantly more computational resources compared to CNNs due to their high number of parameters. Training from scratch demands large-scale datasets and extensive GPU/TPU resources.\n",
        "- Interpretability: Unlike CNNs, which leverage spatial hierarchies, transformers rely on global attention mechanisms, making them even more difficult to interpret.\n",
        "-\tPretraining Dependency: In most practical scenarios, ViTs are pre-trained on massive datasets (e.g., ImageNet-21k, JFT-300M) before being fine-tuned on specific downstream tasks. Training from scratch is feasible but often impractical for smaller datasets.\n",
        "\n",
        "Despite these challenges, ViTs have shown impressive results in image classification, object detection, and segmentation, signaling a shift in deep learning methodologies for vision tasks. In this notebook, we will explore how to build a Vision Transformer from scratch, gaining insights into its structure, training requirements, and performance characteristics."
      ],
      "metadata": {
        "id": "txDUKwFp2O9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Name: Abhishek Agrawal\n",
        "- NetId: aa9360"
      ],
      "metadata": {
        "id": "5-Vb7myu97T3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlCIT7IJ2L0K"
      },
      "outputs": [],
      "source": [
        "!pip install einops\n",
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import time\n",
        "from torchinfo import summary\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvYbWcJji2gZ",
        "outputId": "e9d6ca29-2355-4805-b09a-fa9b1a42737b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.64520577 0.01220981 0.41715171]\n",
            " [0.85467564 0.13222973 0.02282506]]\n",
            "tensor([[0.6452, 0.0122, 0.4172],\n",
            "        [0.8547, 0.1322, 0.0228]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "DOWNLOAD_PATH = './data'\n",
        "BATCH_SIZE_TRAIN = 100\n",
        "BATCH_SIZE_TEST = 1000\n",
        "MEAN = 0.2859\n",
        "STD = 0.3530"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDhu7psgjBVe",
        "outputId": "87d20fbb-0380-4737-e93c-748ef70e1bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[10.6452, 10.0122, 10.4172],\n",
            "        [10.8547, 10.1322, 10.0228]], dtype=torch.float64)\n",
            "\n",
            "tensor([[0.6014, 0.0122, 0.4052],\n",
            "        [0.7544, 0.1318, 0.0228]], dtype=torch.float64)\n",
            "\n",
            "tensor(2.0843, dtype=torch.float64)\n",
            "\n",
            "tensor(0.3474, dtype=torch.float64)\n",
            "\n",
            "torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainingdata = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testdata = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainDataLoader = torch.utils.data.DataLoader(trainingdata,batch_size=64,shuffle=True)\n",
        "testDataLoader = torch.utils.data.DataLoader(testdata,batch_size=64,shuffle=False)"
      ],
      "metadata": {
        "id": "sAqkXbEn_S5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pair(t):\n",
        "return t if isinstance(t, tuple) else (t, t)"
      ],
      "metadata": {
        "id": "jNIiMP2b_qyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.ReLU(),  # Alternative: nn.GELU()\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=64, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.attend = nn.Softmax(dim=-1)\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
        "\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "        attn = self.attend(dots)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
        "            ]) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim,\n",
        "                 pool='cls', channels=3, dim_head=64, dropout=0., emb_dropout=0.):\n",
        "        super().__init__()\n",
        "\n",
        "        assert image_size % patch_size == 0, \"Image dimensions must be divisible by patch size.\"\n",
        "\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * patch_size * patch_size\n",
        "\n",
        "        assert pool in {'cls', 'mean'}, \"pool type must be either 'cls' (CLS token) or 'mean' (mean pooling).\"\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
        "            nn.Linear(patch_dim, dim)\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x[:, 0] if self.pool == 'cls' else x.mean(dim=1)\n",
        "\n",
        "        return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "fWpFppXL_u3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViT(image_size=28, patch_size=4, num_classes=10, channels=1,\n",
        "dim=64, depth=6, heads=4, mlp_dim=128)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "KeqSUVpoAhlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_history = []\n",
        "test_loss_history = []\n",
        "train_accuracy_history = []\n",
        "test_accuracy_history = []\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "num_epochs = 30\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    loop = tqdm(trainDataLoader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\", leave=False)\n",
        "    for images, labels in loop:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predicted_output = model(images)\n",
        "        loss = loss_fn(predicted_output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(predicted_output, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_accuracy = correct_train / total_train\n",
        "    train_loss = train_loss / len(trainDataLoader)\n",
        "\n",
        "    train_loss_history.append(train_loss)\n",
        "    train_accuracy_history.append(train_accuracy)\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        loop = tqdm(testDataLoader, desc=\"Evaluating\", leave=False)\n",
        "        for images, labels in loop:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            predicted_output = model(images)\n",
        "            loss = loss_fn(predicted_output, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(predicted_output, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "    test_accuracy = correct_test / total_test\n",
        "    test_loss = test_loss / len(testDataLoader)\n",
        "\n",
        "    test_loss_history.append(test_loss)\n",
        "    test_accuracy_history.append(test_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} | \"\n",
        "          f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "D-tAXiV2A6TA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}